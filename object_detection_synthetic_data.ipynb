{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Fast.ai, PyTorch and Icevision\n",
    "\n",
    "Initial experiments training FasterRCNN (object detection) with Forensic Architecture's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icevision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data/Canisters_2020\")\n",
    "annotations_dir = data_dir / \"Annotations\"\n",
    "images_dir = data_dir / \"JPEGImages\"\n",
    "imagesets_dir = data_dir / \"ImageSets/Main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Imagesets for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = [(Path(line.rstrip('\\n')).name).split(\".\",1)[0] for line in open(imagesets_dir / \"synth\")]\n",
    "train = [(Path(line.rstrip('\\n')).name).split(\".\",1)[0] for line in open(imagesets_dir / \"train\")]\n",
    "val = [(Path(line.rstrip('\\n')).name).split(\".\",1)[0] for line in open(imagesets_dir / \"val\")]\n",
    "test = [(Path(line.rstrip('\\n')).name).split(\".\",1)[0] for line in open(imagesets_dir / \"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = ClassMap(['canister',\n",
    "                 'cylinder',\n",
    "                 'can',\n",
    "                 'bottle',\n",
    "                 'bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data with icevision voc parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = parsers.voc(\n",
    "    annotations_dir=annotations_dir, images_dir=images_dir, class_map=class_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presplits = [synth, val, test]\n",
    "presplits = [train, val, test]\n",
    "data_splitter = FixedSplitter(presplits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records, valid_records, test_records = parser.parse(data_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our data has been prepared correctly with corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_records(train_records[:2], ncols=2, class_map=class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: we can remove any bboxes at the edge of the image that don't have enough of our object\n",
    "# in view. We judge this based on bbox area relative to objects fully in view and the \n",
    "# ratio of the bbox dimensions. For instance, with a max ratio of 1.6, any bboxes where either \n",
    "# width or height are over 1.6x the other will be removed. For min_area of 0.7, any bbox covering\n",
    "# less than 70% area of the average full bbox will be removed. Although we should be careful as general \n",
    "# advice suggests we should leave objects partially in view!\n",
    "\n",
    "# def remove_partials(records, max_ratio, min_area):\n",
    "#     for record in records:\n",
    "#         partial_bboxes = []\n",
    "#         full_bboxes = []\n",
    "#         for bbox in record.bboxes:\n",
    "#             # if bbox is on the edge of the image, store within partial_bboxes\n",
    "#             if bbox.xmin == 0 or bbox.ymin == 0 or bbox.xmax >= record.width-1 or bbox.ymax >= record.height-1:\n",
    "#                 partial_bboxes.append(bbox)\n",
    "#             else: \n",
    "#                 bbox_area = (bbox.xmax-bbox.xmin)*(bbox.ymax-bbox.ymin)\n",
    "#                 full_bboxes.append(bbox_area)\n",
    "#             # find the average area of a full box\n",
    "#         if full_bboxes: \n",
    "#             mean_bbox_area = sum(full_bboxes)/len(full_bboxes)\n",
    "#         for bbox in partial_bboxes:\n",
    "#             dims = (bbox.xmax-bbox.xmin, bbox.ymax-bbox.ymin)\n",
    "#             if max(dims)/min(dims) < max_ratio: \n",
    "#                 if full_bboxes:\n",
    "#                     bbox_area = (bbox.xmax-bbox.xmin)*(bbox.ymax-bbox.ymin)\n",
    "#                     if bbox_area/mean_bbox_area < min_area: \n",
    "#                         record.labels.pop(record.bboxes.index(bbox))\n",
    "#                         record.bboxes.remove(bbox)\n",
    "#                         print(\"remove:\" + str(bbox))\n",
    "#             else: \n",
    "#                 record.labels.pop(record.bboxes.index(bbox))\n",
    "#                 record.bboxes.remove(bbox)\n",
    "#                 print(\"remove:\" + str(bbox))\n",
    "\n",
    "# remove_partials(train_records, 1.4, 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply transforms to our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 384\n",
    "\n",
    "train_tfms = tfms.A.Adapter(\n",
    "    [*tfms.A.aug_tfms(size=size, presize=None), tfms.A.Normalize(), tfms.A.HorizontalFlip(), tfms.A.Blur(blur_limit=(1, 10)), tfms.A.ShiftScaleRotate()]\n",
    ")\n",
    "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size=size), tfms.A.Normalize()])\n",
    "test_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size=size), tfms.A.Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)\n",
    "test_ds = Dataset(test_records, test_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [train_ds[11] for _ in range(10)]\n",
    "show_samples(samples, denormalize_fn=denormalize_imagenet, ncols=5, display_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = faster_rcnn.train_dl(train_ds, batch_size=32, num_workers=0, shuffle=True)\n",
    "valid_dl = faster_rcnn.valid_dl(test_ds, batch_size=32, num_workers=0, shuffle=False)\n",
    "\n",
    "# train_dl = retinanet.train_dl(train_ds, batch_size=32, num_workers=0, shuffle=True)\n",
    "# valid_dl = retinanet.valid_dl(valid_ds, batch_size=32, num_workers=0, shuffle=False)\n",
    "\n",
    "# train_dl = efficientdet.train_dl(train_ds, batch_size=32, num_workers=0, shuffle=True)\n",
    "# valid_dl = efficientdet.valid_dl(valid_ds, batch_size=32, num_workers=0, shuffle=False)\n",
    "\n",
    "# batch, samples = first(train_dl)\n",
    "# show_samples(\n",
    "#     samples[0:6], ncols=2, denormalize_fn=denormalize_imagenet, display_label=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = efficientdet.model(\n",
    "#     model_name=\"tf_efficientdet_lite0\", num_classes=len(class_map), img_size=size\n",
    "# )\n",
    "\n",
    "# model = retinanet.model(num_classes=len(class_map))\n",
    "\n",
    "model = faster_rcnn.model(len(class_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = efficientdet.fastai.learner(\n",
    "#     dls=[train_dl, valid_dl], model=model, metrics=metrics, detection_threshold = 0.90)\n",
    "\n",
    "# learn = retinanet.fastai.learner(\n",
    "#     dls=[train_dl, valid_dl], model=model, metrics=metrics)\n",
    "\n",
    "learn = faster_rcnn.fastai.learner(\n",
    "    dls=[train_dl, valid_dl], model=model, metrics=metrics, detection_threshold = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(50, 1e-4, freeze_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we're no longer looking for that maximum gradient but somewhere well before the steep rise ie 8e-4\n",
    "learn.freeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminative learning rates: \n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, lr_max=slice(1e-6, 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(),Path('models/fasterrcnn_3_0.386.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and try infer ... \n",
    "state_dict = torch.load('models/fasterrcnn_3_0.386.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unlabelled images for inference from filesystem... \n",
    "import PIL\n",
    "def image_from_file(path): \n",
    "    img = np.array(PIL.Image.open(path))\n",
    "    return img[:,:,:3]\n",
    "\n",
    "imgs = []\n",
    "for file in os.listdir(infer_dir):\n",
    "    img = image_from_file(str(infer_dir) + \"/\" + file)\n",
    "    imgs.append(img)\n",
    "\n",
    "infer_ds = Dataset.from_images(imgs, valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, samples = faster_rcnn.build_infer_batch(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = faster_rcnn.predict(model=model, batch=batch, detection_threshold= 0.3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare groundtruth folder:\n",
    "valid_ds[0]\n",
    "for i in range(len(valid_ds)): \n",
    "    labels = []\n",
    "    for j in range(len(valid_ds[i]['labels'])):\n",
    "        label = valid_ds[i]['labels'][j]\n",
    "        name = class_map.get_id(label)\n",
    "        bbox = valid_ds[i]['bboxes'][j]\n",
    "        line = str(name) + \" \" + str(bbox.xmin) + \" \" + str(bbox.ymin) + \" \" + str(bbox.xmax) + \" \" + str(bbox.ymax)\n",
    "        labels.append(line)\n",
    "    file = \"groundtruths/\" + val[i] + '.txt'\n",
    "    with open(file, 'w') as f: \n",
    "        for item in labels:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare detections folder:\n",
    "for i in range(len(preds)): \n",
    "    detections = []\n",
    "    for j in range(len(preds[i]['labels'])):\n",
    "        label = preds[i]['labels'][j]\n",
    "        name = class_map.get_id(label)\n",
    "        confidence = preds[i]['scores'][j]\n",
    "        bbox = preds[i]['bboxes'][j]\n",
    "        line = str(name) + \" \" + str(confidence) + \" \" + str(bbox.xmin) + \" \" + str(bbox.ymin) + \" \" + str(bbox.xmax) + \" \" + str(bbox.ymax)\n",
    "        detections.append(line)\n",
    "    file = \"detections/\" + val[i] + '.txt'\n",
    "    with open(file, 'w') as f: \n",
    "        for item in detections:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix filenames:\n",
    "import os\n",
    "for file in os.listdir('ml_evaluation/groundtruths'):\n",
    "    file = 'ml_evaluation/groundtruths/' + file\n",
    "    if \"not_can\" in file:\n",
    "        os.rename(file, file.replace(\"_val\",\"\"))\n",
    "    elif \"val_\" in file:\n",
    "        os.rename(file, file.replace(\"val_\",\"test_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show predictions:\n",
    "images = [sample[\"img\"] for sample in samples]\n",
    "show_preds(samples=images[0:10],\n",
    "           preds=preds[0:10],\n",
    "           class_map=class_map,\n",
    "           denormalize_fn=denormalize_imagenet,\n",
    "           ncols=5\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with groundtruths:\n",
    "show_records(valid_ds.records[0:10], ncols=5, class_map=class_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
